{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "# For reading annotations file\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "from collections import OrderedDict\n",
    "import os \n",
    "# For data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from skimage.exposure import is_low_contrast\n",
    "from imutils.paths import list_images\n",
    "import imutils\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blur filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=2.40s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Reading annotations.json\n",
    "TRAIN_ANNOTATIONS_PATH = \"data/train/new_top50cat_train_filter.json\"\n",
    "#TRAIN_ANNOTATIONS_PATH = \"data/train/new_ann_train_filter.json\"\n",
    "TRAIN_IMAGE_DIRECTIORY = \"data/train/images/\"\n",
    "VAL_ANNOTATIONS_PATH = \"data/val/new_top50cat_val_filter.json\"\n",
    "#VAL_ANNOTATIONS_PATH = \"data/val/new_ann_val_filter.json\"\n",
    "VAL_IMAGE_DIRECTIORY = \"data/val/images/\"\n",
    "\n",
    "train_coco = COCO(TRAIN_ANNOTATIONS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the annotation files\n",
    "with open(TRAIN_ANNOTATIONS_PATH) as f:\n",
    "    train_annotations_data = json.load(f)\n",
    "with open(VAL_ANNOTATIONS_PATH) as f:\n",
    "    val_annotations_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'images', 'annotations', 'categories'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotations_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 131094, 'file_name': '131094.jpg', 'width': 480, 'height': 480}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotations_data['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 184135,\n",
       " 'image_id': 131094,\n",
       " 'category_id': 1352,\n",
       " 'segmentation': [[115.0,\n",
       "   206.5,\n",
       "   98.0,\n",
       "   204.5,\n",
       "   74.5,\n",
       "   182.0,\n",
       "   65.0,\n",
       "   167.5,\n",
       "   47.5,\n",
       "   156.0,\n",
       "   39.5,\n",
       "   137.0,\n",
       "   39.5,\n",
       "   130.0,\n",
       "   51.0,\n",
       "   118.5,\n",
       "   62.00000000000001,\n",
       "   112.5,\n",
       "   76.0,\n",
       "   113.5,\n",
       "   121.5,\n",
       "   151.0,\n",
       "   130.5,\n",
       "   169.0,\n",
       "   131.5,\n",
       "   185.0,\n",
       "   128.5,\n",
       "   195.0]],\n",
       " 'area': 5059.0,\n",
       " 'bbox': [39.5, 39.5, 167.0, 92.0],\n",
       " 'iscrowd': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotations_data['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_blur(annotations,image_path,json_path):\n",
    "    new_annotations = dict()\n",
    "    new_annotations['info'] = copy.deepcopy(annotations['info'])\n",
    "    new_annotations['categories'] = copy.deepcopy(annotations['categories'])\n",
    "    blur_threshold = 50 # blur threshold\n",
    "    blur_image_id_list = []\n",
    "    new_image_list = []\n",
    "    new_annotation_list = []\n",
    "    for image in tqdm(annotations['images']):\n",
    "        \n",
    "        file_path = image_path + image['file_name']\n",
    "        image_read = cv2.imread(file_path)\n",
    "        gray = cv2.cvtColor(image_read, cv2.COLOR_BGR2GRAY)\n",
    "        fm = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "        if fm < blur_threshold:\n",
    "            blur_image_id_list.append(image['id'])\n",
    "        else:\n",
    "            new_image_list.append(image)\n",
    "            \n",
    "            \n",
    "    for image in tqdm(new_image_list):\n",
    "        image_id = image['id']\n",
    "        for annotation in annotations['annotations']:\n",
    "            if annotation['image_id'] == image_id:\n",
    "                new_annotation = copy.deepcopy(annotation)\n",
    "                new_annotation_list.append(new_annotation)\n",
    "    new_annotations['images'] = new_image_list\n",
    "    new_annotations['annotations'] = new_annotation_list\n",
    "    print(len(blur_image_id_list))\n",
    "    print(len(new_image_list))\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(new_annotations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 583/583 [00:04<00:00, 129.76it/s]\n",
      "100%|██████████| 507/507 [00:00<00:00, 10349.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "507\n"
     ]
    }
   ],
   "source": [
    "filter_blur(val_annotations_data,\"data/val/images/\",\"data/val/new_top50cat_val_filter_no_blur.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_blur(train_annotations_data,\"data/train/images/\",\"data/train/new_top50cat_train_filter_no_blur.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrast increaing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect low contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39962/39962 [22:54<00:00, 29.08it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_low_contrast_list(data_path):\n",
    "    low_contrast_list = []\n",
    "    for image_info in tqdm(train_annotations_data['images']):\n",
    "        file_path = data_path + image_info['file_name']\n",
    "        image = cv2.imread(file_path)\n",
    "        image = imutils.resize(image, width=450)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "        edged = cv2.Canny(blurred, 30, 150)\n",
    "        if is_low_contrast(gray, fraction_threshold=0.6):\n",
    "            low_contrast_list.append(image_info['file_name'])\n",
    "    return low_contrast_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_contrast_list_train = get_low_contrast_list('data/train/images')\n",
    "low_contrast_list_val = get_low_contrast_list('data/train/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2984"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(low_contrast_list_train))\n",
    "print(len(low_contrast_list_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increas contrast if low contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_contrast(low_contrast_list, data_path):\n",
    "\n",
    "    for image_name in tqdm(low_contrast_list):\n",
    "        file_path = data_path + image_name\n",
    "        image = cv2.imread(file_path)\n",
    "        new_image = np.zeros(image.shape, image.dtype)\n",
    "        alpha = 1.2 # Simple contrast control\n",
    "        beta = 30  # Simple brightness control\n",
    "        new_image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "        # save image(will chage the original image)\n",
    "        cv2.imwrite(file_path, new_image)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improve_contrast(low_contrast_list_train, \"data/train/images/\")\n",
    "improve_contrast(low_contrast_list_val, \"data/val/images/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_envs",
   "language": "python",
   "name": "my_envs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
